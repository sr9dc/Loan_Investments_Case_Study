{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Modeling\n",
    "\n",
    "**Note 1: the following starting code only generates a single random train/test split when default_seed is used. You need to modify the code to generate 100 independent train/test splits with different seeds and report the average results on those independent splits along with standard deviation.**\n",
    "\n",
    "**Note 2: You are completely free to use your own implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load general utilities\n",
    "# ----------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "# Load sklearn utilities\n",
    "# ----------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, brier_score_loss, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Load classifiers\n",
    "# ----------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Other Packages\n",
    "# --------------\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from six import StringIO \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "# from scipy.interpolate import spline\n",
    "\n",
    "# Load debugger, if required\n",
    "#import pixiedust\n",
    "pd.options.mode.chained_assignment = None #'warn'\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that, given a CVGridSearch object, finds the\n",
    "# percentage difference between the best and worst scores\n",
    "def find_score_variation(cv_model):\n",
    "    all_scores = cv_model.cv_results_['mean_test_score']\n",
    "    return( np.abs((max(all_scores) - min(all_scores))) * 100 / max(all_scores) )\n",
    "\n",
    "    '''\n",
    "    which_min_score = np.argmin(all_scores)\n",
    "    \n",
    "    all_perc_diff = []\n",
    "    \n",
    "    try:\n",
    "        all_perc_diff.append( np.abs(all_scores[which_min_score - 1] - all_scores[which_min_score])*100 / min(all_scores) )\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        all_perc_diff.append( np.abs(all_scores[which_min_score + 1] - all_scores[which_min_score])*100 / min(all_scores) )\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return ( np.mean(all_perc_diff) )\n",
    "    '''\n",
    "\n",
    "# Define a function that checks, given a CVGridSearch object,\n",
    "# whether the optimal parameters lie on the edge of the search\n",
    "# grid\n",
    "def find_opt_params_on_edge(cv_model):\n",
    "    out = False\n",
    "    \n",
    "    for i in cv_model.param_grid:\n",
    "        if cv_model.best_params_[i] in [ cv_model.param_grid[i][0], cv_model.param_grid[i][-1] ]:\n",
    "            out = True\n",
    "            break\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a default random seed and an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_seed = 1\n",
    "output_file = \"output_sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to print a line to our output file\n",
    "\n",
    "def dump_to_output(key, value):\n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(\",\".join([str(default_seed), key, str(value)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and engineer the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and features from the pickle file saved in CS-Phase 2\n",
    "data, discrete_features, continuous_features, ret_cols = pickle.load( open( \"clean_data.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the outcome columns: True if loan_status is either Charged Off or Default, False otherwise\n",
    "data[\"outcome\"] = ((data['loan_status'] == 'Charged Off') | (data['loan_status'] == 'Default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature for the length of a person's credit history at the time the loan is issued\n",
    "data['cr_hist'] = (data.issue_d - data.earliest_cr_line) / np.timedelta64(1, 'M')\n",
    "continuous_features.append('cr_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "# Randomly assign each row to a training and test set. We do this now because we will be fitting a variety of models on various time periods, and we would like every period to use the *same* training/test split\n",
    "np.random.seed(default_seed)\n",
    "## create the train columns where the value is True if it is a train instance and False otherwise. Hint: use np.random.choice with 70% for training and 30% for testing\n",
    "indices = np.random.choice(range(len(data)), int(0.7*len(data)), replace=False)\n",
    "\n",
    "data['train'] = False\n",
    "data['train'].iloc[list(indices)] = True\n",
    "\n",
    "print(len(data['train'][data['train'] == False]) / len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of features and outcomes, with dummies. Record the names of the dummies for later use\n",
    "X_continuous = data[continuous_features].values\n",
    "\n",
    "X_discrete = pd.get_dummies(data[discrete_features], dummy_na = True, prefix_sep = \"::\", drop_first = True)\n",
    "discrete_features_dummies = X_discrete.columns.tolist()\n",
    "X_discrete = X_discrete.values\n",
    "\n",
    "X = np.concatenate( (X_continuous, X_discrete), axis = 1 )\n",
    "\n",
    "y = data.outcome.values\n",
    "\n",
    "train = data.train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare functions to fit and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_subset = np.array([True]*len(data)),\n",
    "                    n_samples_train = 30000,\n",
    "                    n_samples_test = 20000,\n",
    "                    feature_subset = None,\n",
    "                    date_range_train = (data.issue_d.min(), data.issue_d.max()),\n",
    "                    date_range_test = (data.issue_d.min(), data.issue_d.max()),\n",
    "                    random_state = default_seed):\n",
    "    '''\n",
    "    This function will prepare the data for classification or regression.\n",
    "    It expects the following parameters:\n",
    "      - data_subset: a numpy array with as many entries as rows in the\n",
    "                     dataset. Each entry should be True if that row\n",
    "                     should be used, or False if it should be ignored\n",
    "      - n_samples_train: the total number of samples to be used for training.\n",
    "                         Will trigger an error if this number is larger than\n",
    "                         the number of rows available after all filters have\n",
    "                         been applied\n",
    "      - n_samples_test: as above for testing\n",
    "      - feature_subect: A list containing the names of the features to be\n",
    "                        used in the model. In None, all features in X are\n",
    "                        used\n",
    "      - date_range_train: a tuple containing two dates. All rows with loans\n",
    "                          issued outside of these two dates will be ignored in\n",
    "                          training\n",
    "      - date_range_test: as above for testing\n",
    "      - random_state: the random seed to use when selecting a subset of rows\n",
    "      \n",
    "    Note that this function assumes the data has a \"Train\" column, and will\n",
    "    select all training rows from the rows with \"True\" in that column, and all\n",
    "    the testing rows from those with a \"False\" in that column.\n",
    "    \n",
    "    This function returns a dictionary with the following entries\n",
    "      - X_train: the matrix of training data\n",
    "      - y_train: the array of training labels\n",
    "      - train_set: a Boolean vector with as many entries as rows in the data\n",
    "                  that denotes the rows that were used in the train set\n",
    "      - X_test: the matrix of testing data\n",
    "      - y_test: the array of testing labels\n",
    "      - test_set: a Boolean vector with as many entries as rows in the data\n",
    "                  that denotes the rows that were used in the test set\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    # Filter down the data to the required date range, and downsample\n",
    "    # as required\n",
    "    filter_train = ( train & (data.issue_d >= date_range_train[0]) &\n",
    "                            (data.issue_d <= date_range_train[1]) & data_subset ).values\n",
    "    filter_test = ( (train == False) & (data.issue_d >= date_range_test[0])\n",
    "                            & (data.issue_d <= date_range_test[1]) & data_subset ).values\n",
    "    \n",
    "    filter_train[ np.random.choice( np.where(filter_train)[0], size = filter_train.sum()\n",
    "                                                   - n_samples_train, replace = False ) ] = False\n",
    "    filter_test[ np.random.choice( np.where(filter_test)[0], size = filter_test.sum()\n",
    "                                                   - n_samples_test, replace = False ) ] = False\n",
    "    \n",
    "    # Prepare the training and test set\n",
    "    X_train = X[ filter_train , :]\n",
    "    X_test = X[ filter_test, :]\n",
    "    if feature_subset != None:\n",
    "        cols = [i for i, j in enumerate(continuous_features + discrete_features_dummies)\n",
    "                                                     if j.split(\"::\")[0] in feature_subset]\n",
    "        X_train = X_train[ : , cols ]\n",
    "        X_test = X_test[ : , cols ]\n",
    "        \n",
    "    y_train = y[ filter_train ]\n",
    "    y_test = y[ filter_test ]\n",
    "    \n",
    "    # Scale the variables\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # return training and testing data\n",
    "    out = {'X_train':X_train, 'y_train':y_train, 'train_set':filter_train, \n",
    "           'X_test':X_test, 'y_test':y_test, 'test_set':filter_test}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_classification(model, data_dict,\n",
    "                          cv_parameters = {},\n",
    "                          model_name = None,\n",
    "                          random_state = default_seed,\n",
    "                          output_to_file = True,\n",
    "                          print_to_screen = True):\n",
    "    '''\n",
    "    This function will fit a classification model to data and print various evaluation\n",
    "    measures. It expects the following parameters\n",
    "      - model: an sklearn model object\n",
    "      - data_dict: the dictionary containing both training and testing data;\n",
    "                   returned by the prepare_data function\n",
    "      - cv_parameters: a dictionary of parameters that should be optimized\n",
    "                       over using cross-validation. Specifically, each named\n",
    "                       entry in the dictionary should correspond to a parameter,\n",
    "                       and each element should be a list containing the values\n",
    "                       to optimize over\n",
    "      - model_name: the name of the model being fit, for printouts\n",
    "      - random_state: the random seed to use\n",
    "      - output_to_file: if the results will be saved to the output file\n",
    "      - print_to_screen: if the results will be printed on screen\n",
    "    \n",
    "    If the model provided does not have a predict_proba function, we will\n",
    "    simply print accuracy diagnostics and return.\n",
    "    \n",
    "    If the model provided does have a predict_proba function, we first\n",
    "    figure out the optimal threshold that maximizes the accuracy and\n",
    "    print out accuracy diagnostics. We then print an ROC curve, sensitivity/\n",
    "    specificity curve, and calibration curve.\n",
    "    \n",
    "    This function returns a dictionary with the following entries\n",
    "      - model: the best fitted model\n",
    "      - y_pred: predictions for the test set\n",
    "      - y_pred_probs: probability predictions for the test set, if the model\n",
    "                      supports them\n",
    "      - y_pred_score: prediction scores for the test set, if the model does not \n",
    "                      output probabilities.\n",
    "    '''\n",
    "        \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # --------------------------\n",
    "    #   Step 1 - Load the data\n",
    "    # --------------------------\n",
    "    X_train = data_dict['X_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    \n",
    "    X_test = data_dict['X_test']\n",
    "    y_test = data_dict['y_test']\n",
    "    \n",
    "    filter_train = data_dict['train_set']    \n",
    "  \n",
    "    # --------------------------\n",
    "    #   Step 2 - Fit the model\n",
    "    # --------------------------\n",
    "\n",
    "    cv_model = GridSearchCV(model, cv_parameters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cv_model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    best_model = cv_model.best_estimator_\n",
    "    \n",
    "    if print_to_screen:\n",
    "\n",
    "        if model_name != None:\n",
    "            print(\"=========================================================\")\n",
    "            print(\"  Model: \" + model_name)\n",
    "            print(\"=========================================================\")\n",
    "\n",
    "        print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "        print(\"Optimal parameters:\")\n",
    "        print(cv_model.best_params_)\n",
    "        print(\"\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    #   Step 3 - Evaluate the model\n",
    "    # -------------------------------\n",
    "    \n",
    "    # If possible, make probability predictions\n",
    "    try:\n",
    "        y_pred_probs = best_model.predict_proba(X_test)[:,1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "        \n",
    "        probs_predicted = True\n",
    "    except:\n",
    "        probs_predicted = False\n",
    "    \n",
    "    # Make predictions; if we were able to find probabilities, use\n",
    "    # the threshold that maximizes the accuracy in the training set.\n",
    "    # If not, just use the learner's predict function\n",
    "    if probs_predicted:\n",
    "        y_train_pred_probs = best_model.predict_proba(X_train)[:,1]\n",
    "        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_pred_probs)\n",
    "        \n",
    "        true_pos_train = tpr_train*(y_train.sum())\n",
    "        true_neg_train = (1 - fpr_train) *(1-y_train).sum()\n",
    "        \n",
    "        best_threshold_index = np.argmax(true_pos_train + true_neg_train)\n",
    "        best_threshold = 1 if best_threshold_index == 0 else thresholds_train[ best_threshold_index ]\n",
    "        \n",
    "        if print_to_screen:\n",
    "            print(\"Accuracy-maximizing threshold was: \" + str(best_threshold))\n",
    "        \n",
    "        y_pred = (y_pred_probs > best_threshold)\n",
    "    else:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    if print_to_screen:\n",
    "        print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred, target_names =['No default', 'Default'], digits = 4))\n",
    "\n",
    "    if print_to_screen:\n",
    "        if probs_predicted:        \n",
    "            plt.figure(figsize = (13, 4.5))\n",
    "            plt.subplot(2, 2, 1)\n",
    "\n",
    "            plt.title(\"ROC Curve (AUC = %0.2f)\"% roc_auc_score(y_test, y_pred_probs))\n",
    "            plt.plot(fpr, tpr, 'b')\n",
    "            plt.plot([0,1],[0,1],'r--')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "\n",
    "            plt.plot(thresholds, tpr, 'b', label = 'Sensitivity')\n",
    "            plt.plot(thresholds, 1 -fpr, 'r', label = 'Specificity')\n",
    "            plt.legend(loc = 'lower right')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.xlabel('Threshold')\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "\n",
    "            fp_0, mpv_0 = calibration_curve(y_test, y_pred_probs, n_bins = 10)\n",
    "            plt.plot([0,1], [0,1], 'k:', label='Perfectly calibrated')\n",
    "            plt.plot(mpv_0, fp_0, 's-')\n",
    "            plt.ylabel('Fraction of Positives')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.legend(loc ='upper left')\n",
    "            \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.hist(y_pred_probs, range=(0, 1), bins=10, histtype=\"step\", lw=2)\n",
    "            plt.xlim([0,1]); plt.ylim([0,20000])\n",
    "            plt.xlabel('Mean Predicted Probability')\n",
    "            plt.ylabel('Count')\n",
    "            \n",
    "            #plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    # Additional Score Check\n",
    "    if probs_predicted:\n",
    "        y_train_score = y_train_pred_probs\n",
    "    else:\n",
    "        y_train_score = best_model.decision_function(X_train)\n",
    "        \n",
    "    tau, p_value = kendalltau(y_train_score, data.grade[filter_train])\n",
    "    if print_to_screen:\n",
    "        print(\"\")\n",
    "        print(\"Similarity to LC grade ranking: \", tau)\n",
    "    \n",
    "    if probs_predicted:\n",
    "        brier_score = brier_score_loss(y_test, y_pred_probs)\n",
    "        if print_to_screen:\n",
    "            print(\"Brier score:\", brier_score)\n",
    "    \n",
    "    # Return the model predictions, and the\n",
    "    # test set\n",
    "    # -------------------------------------\n",
    "    out = {'model':best_model, 'y_pred_labels':y_pred}\n",
    "    \n",
    "    if probs_predicted:\n",
    "        out.update({'y_pred_probs':y_pred_probs})\n",
    "    else:\n",
    "        y_pred_score = best_model.decision_function(X_test)\n",
    "        out.update({'y_pred_score':y_pred_score})\n",
    "        \n",
    "    # Output results to file\n",
    "    # ----------------------\n",
    "    if probs_predicted and output_to_file:\n",
    "        # Check whether any of the CV parameters are on the edge of\n",
    "        # the search space\n",
    "        opt_params_on_edge = find_opt_params_on_edge(cv_model)\n",
    "        dump_to_output(model_name + \"::search_on_edge\", opt_params_on_edge)\n",
    "        if print_to_screen:\n",
    "            print(\"Were parameters on edge? : \" + str(opt_params_on_edge))\n",
    "        \n",
    "        # Find out how different the scores are for the different values\n",
    "        # tested for by cross-validation. If they're not too different, then\n",
    "        # even if the parameters are off the edge of the search grid, we should\n",
    "        # be ok\n",
    "        score_variation = find_score_variation(cv_model)\n",
    "        dump_to_output(model_name + \"::score_variation\", score_variation)\n",
    "        if print_to_screen:\n",
    "            print(\"Score variations around CV search grid : \" + str(score_variation))\n",
    "        \n",
    "        # Print out all the scores\n",
    "        dump_to_output(model_name + \"::all_cv_scores\", str(cv_model.cv_results_['mean_test_score']))\n",
    "        if print_to_screen:\n",
    "            print( str(cv_model.cv_results_['mean_test_score']) )\n",
    "        \n",
    "        # Dump the AUC to file\n",
    "        dump_to_output(model_name + \"::roc_auc\", roc_auc_score(y_test, y_pred_probs) )\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test different machine learning classification models\n",
    "\n",
    "The machine learning models listed in the following are just our suggestions. You are free to try any other models that you would like to experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:pink\">I messed around with this to test prepare_data. -Sai\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define your set of features to use in different models\n",
    "\n",
    "#### we might need to remove some of these features- Sai\n",
    "your_features = ['id', 'loan_amnt', \n",
    "                'funded_amnt', 'term', 'int_rate', 'grade', 'emp_length', 'home_ownership',\n",
    "                'annual_inc', 'verification_status', 'issue_d', 'loan_status', 'purpose','dti',\n",
    "                'delinq_2yrs', 'earliest_cr_line', 'open_acc', 'pub_rec', 'fico_range_high',\n",
    "                'fico_range_low', 'revol_bal', 'revol_util', 'total_pymnt', 'recoveries', 'last_pymnt_d']\n",
    "\n",
    "\n",
    "# prepare the train, test data for training models\n",
    "data_dict = prepare_data(feature_subset = your_features)\n",
    "\n",
    "all_features = pd.Series(continuous_features + discrete_features_dummies)\n",
    "idx = [i for i, j in enumerate(continuous_features + discrete_features_dummies)\n",
    "                                                     if j.split(\"::\")[0] in your_features]\n",
    "selected_features = all_features[idx]\n",
    "selected_features.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.158689</td>\n",
       "      <td>0.393431</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.270664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.158974</td>\n",
       "      <td>0.158974</td>\n",
       "      <td>0.462993</td>\n",
       "      <td>0.111314</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.188172</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.231403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.028205</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>0.087088</td>\n",
       "      <td>0.359489</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.169555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.130049</td>\n",
       "      <td>0.357847</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026882</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.114651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.199645</td>\n",
       "      <td>0.117518</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.215329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.362752</td>\n",
       "      <td>0.375182</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134409</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0.203205</td>\n",
       "      <td>0.203205</td>\n",
       "      <td>0.105346</td>\n",
       "      <td>0.501825</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.081291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.187330</td>\n",
       "      <td>0.396168</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.042810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.695696</td>\n",
       "      <td>0.075182</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.087053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0.246795</td>\n",
       "      <td>0.246795</td>\n",
       "      <td>0.461915</td>\n",
       "      <td>0.289051</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.577888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3       4         5    6   \\\n",
       "0      0.512821  0.512821  0.158689  0.393431  0.0000  0.230769  0.0   \n",
       "1      0.158974  0.158974  0.462993  0.111314  0.0000  0.230769  0.0   \n",
       "2      0.028205  0.028205  0.087088  0.359489  0.0000  0.423077  0.0   \n",
       "3      0.358974  0.358974  0.130049  0.357847  0.0000  0.346154  0.0   \n",
       "4      0.064103  0.064103  0.199645  0.117518  0.0625  0.192308  0.0   \n",
       "...         ...       ...       ...       ...     ...       ...  ...   \n",
       "29995  0.487179  0.487179  0.362752  0.375182  0.0625  0.384615  0.0   \n",
       "29996  0.203205  0.203205  0.105346  0.501825  0.0000  0.423077  0.0   \n",
       "29997  0.025641  0.025641  0.187330  0.396168  0.0000  0.269231  0.0   \n",
       "29998  0.230769  0.230769  0.695696  0.075182  0.0000  0.269231  0.0   \n",
       "29999  0.246795  0.246795  0.461915  0.289051  0.0000  0.346154  0.0   \n",
       "\n",
       "             7         8         9   ...   46   47   48   49   50   51   52  \\\n",
       "0      0.107527  0.108108  0.270664  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "1      0.188172  0.189189  0.231403  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "2      0.161290  0.162162  0.169555  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "3      0.026882  0.027027  0.114651  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "4      0.295699  0.297297  0.215329  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "29995  0.134409  0.135135  0.110434  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "29996  0.295699  0.297297  0.081291  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "29997  0.564516  0.567568  0.042810  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "29998  0.483871  0.486486  0.087053  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "29999  0.403226  0.405405  0.577888  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        53   54   55  \n",
       "0      0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  \n",
       "...    ...  ...  ...  \n",
       "29995  0.0  0.0  0.0  \n",
       "29996  0.0  0.0  0.0  \n",
       "29997  0.0  0.0  0.0  \n",
       "29998  0.0  0.0  0.0  \n",
       "29999  0.0  0.0  0.0  \n",
       "\n",
       "[30000 rows x 56 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### testing function\n",
    "pd.DataFrame(data_dict['X_train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a naive bayes classifier\n",
    "\n",
    "gnb = ...\n",
    "gnb = fit_classification(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_1$ regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a l_1 regularized logistic regression classifier\n",
    "\n",
    "l1_logistic = ...\n",
    "cv_parameters = ...\n",
    "\n",
    "l1_logistic = fit_classification(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_2$ regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a l_1 regularized logistic regression classifier\n",
    "\n",
    "l2_logistic = ...\n",
    "cv_parameters = ...\n",
    "\n",
    "l2_logistic = fit_classification(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot top 3 features with the most positive (and negative) weights \n",
    "top_and_bottom_idx = list(np.argsort(l2_logistic['model'].coef_)[0,:3]) + list(np.argsort(l2_logistic['model'].coef_)[0,-3:])\n",
    "bplot = pd.Series(l2_logistic['model'].coef_[0,top_and_bottom_idx])\n",
    "xticks = selected_features[top_and_bottom_idx]\n",
    "p1 = bplot.plot(kind='bar',rot=-30,ylim=(-5,10))\n",
    "p1.set_xticklabels(xticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a decision tree classifier\n",
    "\n",
    "decision_tree = ...\n",
    "cv_parameters = ...\n",
    "\n",
    "decision_tree = fit_classification(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a random forest classifier\n",
    "\n",
    "random_forest = ...\n",
    "cv_parameters = ...\n",
    "\n",
    "random_forest = fit_classification(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot top 6 most significant features\n",
    "top_idx = list(np.argsort(random_forest['model'].feature_importances_)[-6:]) \n",
    "bplot = pd.Series(random_forest['model'].feature_importances_[top_idx])\n",
    "xticks = selected_features[top_idx]\n",
    "p2 = bplot.plot(kind='bar',rot=-30,ylim=(0,0.2))\n",
    "p2.set_xticklabels(xticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a multi-layer perceptron classifier\n",
    "\n",
    "mlp = ...\n",
    "cv_parameters = ...\n",
    "\n",
    "mlp = fit_classification(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test logistic regression model with features derived by LendingClub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find a lendingClub-defined feature and train a l1-regularized logistic regression model on data with only that feature\n",
    "a_lendingclub_feature = ...\n",
    "\n",
    "data_dict = prepare_data(feature_subset = a_lendingclub_feature)\n",
    "lc1_only_logistic = ...\n",
    "\n",
    "lc1_only_logistic = fit_classification(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train a l2-regularized logistic regression model on data with only that feature\n",
    "lc2_only_logistic = ...\n",
    "\n",
    "lc2_only_logistic = fit_classification(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test all the models you have tried previously after removing features derived by LendingClub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time stability test of YOURMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the time window of your train and test data\n",
    "start_date_train = ...\n",
    "end_date_train = ...\n",
    "start_date_test = ...\n",
    "end_date_test = ...\n",
    "\n",
    "data_dict_test = prepare_data(date_range_train = (start_date_train, end_date_train), \n",
    "                         date_range_test = (start_date_test, end_date_test),\n",
    "                         n_samples_train = 9000, n_samples_test = 7000, feature_subset = your_features)\n",
    "\n",
    "## Train and test YOURMODEL using this data\n",
    "cv_parameters = ...\n",
    "\n",
    "fit_classification(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test YOURMODEL on the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regression(model, data_dict,\n",
    "                      cv_parameters = {},\n",
    "                      separate = False, \n",
    "                      model_name = None,\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = True,\n",
    "                      print_to_screen = True):\n",
    "    '''\n",
    "    This function will fit a regression model to data and print various evaluation\n",
    "    measures. It expects the following parameters\n",
    "      - model: an sklearn model object\n",
    "      - data_dict: the dictionary containing both training and testing data;\n",
    "                   returned by the prepare_data function\n",
    "      - separate: a Boolean variable indicating whether we fit models for \n",
    "                  defaulted and non-defaulted loans separately\n",
    "      - cv_parameters: a dictionary of parameters that should be optimized\n",
    "                       over using cross-validation. Specifically, each named\n",
    "                       entry in the dictionary should correspond to a parameter,\n",
    "                       and each element should be a list containing the values\n",
    "                       to optimize over      \n",
    "      - model_name: the name of the model being fit, for printouts\n",
    "      - random_state: the random seed to use\n",
    "      - output_to_file: if the results will be saved to the output file\n",
    "      - print_to_screen: if the results will be printed on screen\n",
    "    \n",
    "    This function returns a dictionary FOR EACH RETURN DEFINITION with the following entries\n",
    "      - model: the best fitted model\n",
    "      - predicted_return: prediction result based on the test set\n",
    "      - predicted_regular_return: prediction result for non-defaulted loans (valid if separate == True)\n",
    "      - predicted_default_return: prediction result for defaulted loans (valid if separate == True)\n",
    "      - r2_scores: the testing r2_score(s) for the best fitted model\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # --------------------------\n",
    "    #   Step 1 - Load the data\n",
    "    # --------------------------\n",
    "    \n",
    "    col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "    \n",
    "    X_train = data_dict['X_train']\n",
    "    filter_train = data_dict['train_set']  \n",
    "\n",
    "    X_test = data_dict['X_test']\n",
    "    filter_test = data_dict['test_set']\n",
    "    out = {}\n",
    "    \n",
    "    for ret_col in col_list:\n",
    "        \n",
    "        y_train = data.loc[filter_train, ret_col].as_matrix()\n",
    "        y_test = data.loc[filter_test, ret_col].as_matrix() \n",
    "\n",
    "        # --------------------------\n",
    "        #   Step 2 - Fit the model\n",
    "        # --------------------------\n",
    "\n",
    "        if separate:\n",
    "            outcome_train = data.loc[filter_train, 'outcome']\n",
    "            outcome_test = data.loc[filter_test, 'outcome']\n",
    "\n",
    "            # Train two separate regressors for defaulted and non-defaulted loans\n",
    "            X_train_0 = X_train[outcome_train == False]\n",
    "            y_train_0 = y_train[outcome_train == False]\n",
    "            X_test_0 = X_test[outcome_test == False]\n",
    "            y_test_0 = y_test[outcome_test == False]\n",
    "\n",
    "            X_train_1 = X_train[outcome_train == True]\n",
    "            y_train_1 = y_train[outcome_train == True]\n",
    "            X_test_1 = X_test[outcome_test == True]\n",
    "            y_test_1 = y_test[outcome_test == True]\n",
    "\n",
    "            cv_model_0 = GridSearchCV(model, cv_parameters, scoring='r2')\n",
    "            cv_model_1 = GridSearchCV(model, cv_parameters, scoring='r2')\n",
    "\n",
    "            start_time = time.time()\n",
    "            cv_model_0.fit(X_train_0, y_train_0)\n",
    "            cv_model_1.fit(X_train_1, y_train_1)\n",
    "            end_time = time.time()\n",
    "\n",
    "            best_model_0 = cv_model_0.best_estimator_\n",
    "            best_model_1 = cv_model_1.best_estimator_\n",
    "            \n",
    "            if print_to_screen:\n",
    "\n",
    "                if model_name != None:\n",
    "                    print(\"=========================================================\")\n",
    "                    print(\"  Model: \" + model_name + \"  Return column: \" + ret_col)\n",
    "                    print(\"=========================================================\")\n",
    "\n",
    "                print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "                print(\"Optimal parameters:\")\n",
    "                print(\"model_0:\",cv_model_0.best_params_, \"model_1\",cv_model_1.best_params_)\n",
    "\n",
    "            predicted_regular_return = best_model_0.predict(X_test)\n",
    "            predicted_default_return = best_model_1.predict(X_test)\n",
    "            \n",
    "            if print_to_screen:\n",
    "                print(\"\")\n",
    "                print(\"Testing r2 scores:\")\n",
    "            # Here we use different testing set to report the performance\n",
    "            test_scores = {'model_0':r2_score(y_test_0,best_model_0.predict(X_test_0)),\n",
    "                              'model_1':r2_score(y_test_1,best_model_1.predict(X_test_1))}\n",
    "            if print_to_screen:\n",
    "                print(\"model_0:\", test_scores['model_0'])\n",
    "                print(\"model_1:\", test_scores['model_1'])\n",
    "\n",
    "            cv_objects = {'model_0':cv_model_0, 'model_1':cv_model_1}\n",
    "            out[ret_col] = { 'model_0':best_model_0, 'model_1':best_model_1, 'predicted_regular_return':predicted_regular_return,\n",
    "                      'predicted_default_return':predicted_default_return,'r2_scores':test_scores }\n",
    "\n",
    "        else:\n",
    "            cv_model = GridSearchCV(model, cv_parameters, scoring='r2')\n",
    "\n",
    "            start_time = time.time()\n",
    "            cv_model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "\n",
    "            best_model = cv_model.best_estimator_\n",
    "            \n",
    "            if print_to_screen:\n",
    "                if model_name != None:\n",
    "                    print(\"=========================================================\")\n",
    "                    print(\"  Model: \" + model_name + \"  Return column: \" + ret_col)\n",
    "                    print(\"=========================================================\")\n",
    "\n",
    "                print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "                print(\"Optimal parameters:\")\n",
    "                print(cv_model.best_params_)\n",
    "\n",
    "            predicted_return = best_model.predict(X_test)\n",
    "            test_scores = {'model':r2_score(y_test,predicted_return)}\n",
    "            if print_to_screen:\n",
    "                print(\"\")\n",
    "                print(\"Testing r2 score:\", test_scores['model'])\n",
    "\n",
    "            cv_objects = {'model':cv_model}\n",
    "            out[ret_col] = {'model':best_model, 'predicted_return':predicted_return, 'r2_scores':r2_score(y_test,predicted_return)}\n",
    "\n",
    "        # Output the results to a file\n",
    "        if output_to_file:\n",
    "            for i in cv_objects:\n",
    "                # Check whether any of the CV parameters are on the edge of\n",
    "                # the search space\n",
    "                opt_params_on_edge = find_opt_params_on_edge(cv_objects[i])\n",
    "                dump_to_output(model_name + \"::\" + ret_col + \"::search_on_edge\", opt_params_on_edge)\n",
    "                if print_to_screen:\n",
    "                    print(\"Were parameters on edge (\" + i + \") : \" + str(opt_params_on_edge))\n",
    "\n",
    "                # Find out how different the scores are for the different values\n",
    "                # tested for by cross-validation. If they're not too different, then\n",
    "                # even if the parameters are off the edge of the search grid, we should\n",
    "                # be ok\n",
    "                score_variation = find_score_variation(cv_objects[i])\n",
    "                dump_to_output(model_name + \"::\" + ret_col + \"::score_variation\", score_variation)\n",
    "                if print_to_screen:\n",
    "                    print(\"Score variations around CV search grid (\" + i + \") : \" + str(score_variation))\n",
    "\n",
    "                # Print out all the scores\n",
    "                dump_to_output(model_name + \"::all_cv_scores\", str(cv_objects[i].cv_results_['mean_test_score']))\n",
    "                if print_to_screen:\n",
    "                    print(\"All test scores : \" + str(cv_objects[i].cv_results_['mean_test_score']) )\n",
    "\n",
    "                # Dump the AUC to file\n",
    "                dump_to_output( model_name + \"::\" + ret_col + \"::r2\", test_scores[i] )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_1$ regularized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, trying l1 regularized linear regression with hyper-parameters\n",
    "\n",
    "cv_parameters = ...\n",
    "\n",
    "reg_lasso = fit_regression(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_2$ regularized linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying l2 regularized linear regression with hyper-parameters\n",
    "\n",
    "cv_parameters = ...\n",
    "\n",
    "reg_ridge = fit_regression(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying multi-layer perceptron regression with hyper-parameters\n",
    "\n",
    "cv_parameters = ...\n",
    "\n",
    "reg_mlp = fit_regression(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying random forest regression with hyper-parameters\n",
    "\n",
    "cv_parameters = ...\n",
    "\n",
    "reg_rf = fit_regression(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test investment strategies \n",
    "Now we test several investment strategies using the learning models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_investments(data_dict,\n",
    "                        classifier = None,\n",
    "                        regressor = None,\n",
    "                        strategy = 'Random', \n",
    "                        num_loans = 1000,\n",
    "                        random_state = default_seed,\n",
    "                        output_to_file = True):\n",
    "    '''\n",
    "    This function tests a variety of investment methodologies and their returns. \n",
    "    It will run its tests on the loans defined by the test_set element of the data\n",
    "    dictionary.\n",
    "    \n",
    "    It is currently able to test four strategies\n",
    "      - random: invest in a random set of loans\n",
    "      - default-based: score each loan by probability of default, and only invest\n",
    "                 in the \"safest\" loans (i.e., those with the lowest probabilities\n",
    "                 of default)\n",
    "      - return-based: train a single regression model to predict the expected return\n",
    "                    of loans in the past. Then, for loans we could invest in, simply\n",
    "                    rank them by their expected returns and invest in that order.\n",
    "      - default-& return-based: train two regression models to predict the expected return of\n",
    "                   defaulted loans and non-defaulted loans in the training set. Then,\n",
    "                   for each potential loan we could invest in, predict the probability\n",
    "                   the loan will default, its return if it doesn't default and its\n",
    "                   return if it does. Then, calculate a weighted combination of\n",
    "                   the latter using the former to find a predicted return. Rank the\n",
    "                   loans by this expected return, and invest in that order\n",
    "    \n",
    "    It expects the following parameters\n",
    "      - data_dict: the dictionary containing both training and testing data;\n",
    "                   returned by the prepare_data function\n",
    "      - classifier: a fitted model object which is returned by the fit_classification function.\n",
    "      - regressor: a fitted model object which is returned by the fit_regression function.\n",
    "      - strategy: the name of the strategy; one of the three listed above\n",
    "      - num_loans: the number of loans to be included in the test portfolio\n",
    "      - num_samples: the number of random samples used to compute average return ()   \n",
    "      - random_state: the random seed to use when selecting a subset of rows\n",
    "      - output_to_file: if the results will be saved to the output file\n",
    "      \n",
    "    The function returns a dictionary FOR EACH RETURN DEFINITION with the following entries\n",
    "      - strategy: the name of the strategy\n",
    "      - average return: the return of the strategy based on the testing set\n",
    "      - test data: the updated Dataframe of testing data. Useful in the optimization section\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Retrieve the rows that were used to train and test  the\n",
    "    # classification model\n",
    "    train_set = data_dict['train_set']\n",
    "    test_set = data_dict['test_set']\n",
    "    \n",
    "    col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "    \n",
    "    # Create a dataframe for testing, including the score\n",
    "    data_test = data.loc[test_set,:]\n",
    "    out = {}\n",
    "    \n",
    "    for ret_col in col_list:    \n",
    "    \n",
    "        if strategy == 'Random':\n",
    "            # Randomize the order of the rows in the datframe\n",
    "            data_test = data_test.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "            ## Select num_loans to invest in\n",
    "            pf_test = ...\n",
    "\n",
    "            ## Find the average return for these loans\n",
    "            ret_test = ...\n",
    "\n",
    "            # Return\n",
    "            out[ret_col] = {'strategy':strategy, 'average return':ret_test}\n",
    "\n",
    "            # Dump the strategy performance to file\n",
    "            if output_to_file:\n",
    "                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n",
    "\n",
    "            continue\n",
    "        \n",
    "        elif strategy == 'Return-based':\n",
    "            \n",
    "            colname = 'predicted_return_' + ret_col \n",
    "\n",
    "            data_test[colname] = regressor[ret_col]['predicted_return']\n",
    "\n",
    "            # Sort the loans by predicted return\n",
    "            data_test = data_test.sort_values(by=colname, ascending = False).reset_index(drop = True)\n",
    "\n",
    "            ## Pick num_loans loans\n",
    "            pf_test = ...\n",
    "\n",
    "            ## Find their return\n",
    "            ret_test = ...\n",
    "\n",
    "            # Return\n",
    "            out[ret_col] = {'strategy':strategy, 'average return':ret_test, 'test data':data_test}\n",
    "\n",
    "            # Dump the strategy performance to file\n",
    "            if output_to_file:\n",
    "                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n",
    "\n",
    "            continue\n",
    "            \n",
    "        # Get the predicted scores, if the strategy is not Random or just Regression\n",
    "        try:\n",
    "            y_pred_score = classifier['y_pred_probs']\n",
    "        except:\n",
    "            y_pred_score = classifier['y_pred_score']\n",
    "\n",
    "        data_test['score'] = y_pred_score\n",
    "\n",
    "\n",
    "        if strategy == 'Default-based':\n",
    "            # Sort the test data by the score\n",
    "            data_test = data_test.sort_values(by='score').reset_index(drop = True)\n",
    "\n",
    "            ## Select num_loans to invest in\n",
    "            pf_test = ...\n",
    "\n",
    "            ## Find the average return for these loans\n",
    "            ret_test = ...\n",
    "\n",
    "            # Return\n",
    "            out[ret_col] = {'strategy':strategy, 'average return':ret_test}\n",
    "\n",
    "            # Dump the strategy performance to file\n",
    "            if output_to_file:\n",
    "                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        elif strategy == 'Default-return-based':\n",
    "\n",
    "            # Load the predicted returns\n",
    "            data_test['predicted_regular_return'] = regressor[ret_col]['predicted_regular_return']\n",
    "            data_test['predicted_default_return'] = regressor[ret_col]['predicted_default_return']\n",
    "\n",
    "            # Compute expectation\n",
    "            colname = 'predicted_return_' + ret_col \n",
    "            \n",
    "            data_test[colname] = ( (1-data_test.score)*data_test.predicted_regular_return + \n",
    "                                             data_test.score*data_test.predicted_default_return )\n",
    "\n",
    "            # Sort the loans by predicted return\n",
    "            data_test = data_test.sort_values(by=colname, ascending = False).reset_index(drop = True)\n",
    "\n",
    "            ## Pick num_loans loans\n",
    "            pf_test = ...\n",
    "\n",
    "            ## Find their return\n",
    "            ret_test = ...\n",
    "\n",
    "            # Return\n",
    "            out[ret_col] = {'strategy':strategy, 'average return':ret_test, 'test data':data_test}\n",
    "\n",
    "            # Dump the strategy performance to file\n",
    "            if output_to_file:\n",
    "                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n",
    "\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            return 'Not a valid strategy'\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test investment strategies using the best performing regressor\n",
    "\n",
    "col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "test_strategy = 'Random'\n",
    "\n",
    "print('strategy:',test_strategy)   \n",
    "strat_rand = test_investments(...)\n",
    "\n",
    "for ret_col in col_list:\n",
    "    print(ret_col + ': ' + str(strat_rand[ret_col]['average return']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy = 'Default-based'\n",
    "\n",
    "print('strategy:',test_strategy)\n",
    "strat_def = test_investments(...)\n",
    "\n",
    "for ret_col in col_list:\n",
    "    print(ret_col + ': ' + str(strat_def[ret_col]['average return']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy = 'Return-based'\n",
    "\n",
    "print('strategy:',test_strategy)\n",
    "strat_ret = test_investments(...)\n",
    "\n",
    "for ret_col in col_list:\n",
    "    print(ret_col + ': ' + str(strat_ret[ret_col]['average return']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy = 'Default-return-based'\n",
    "\n",
    "## For the Default-return-based strategy we need to fit a new regressor with separate = True\n",
    "cv_parameters = ...\n",
    "\n",
    "reg_separate = fit_regression(...)\n",
    "\n",
    "print('strategy:',test_strategy)\n",
    "strat_defret = test_investments(...)\n",
    "\n",
    "for ret_col in col_list:\n",
    "    print(ret_col + ': ' + str(strat_defret[ret_col]['average return']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity test of portfolio size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the best-performing data-driven strategy on different portfolio sizes\n",
    "\n",
    "result_sensitivity = []\n",
    "\n",
    "## Vary the portfolio size from 1,000 to 10,000\n",
    "for num_loans in list(range(1000,10000,1000)):\n",
    "\n",
    "    reg_0 = test_investments(...)\n",
    "    result_sensitivity.append(reg_0['ret_PESS']['average return'])\n",
    "    \n",
    "result_sensitivity = np.array(result_sensitivity) * 100\n",
    "sns.pointplot(np.array(list(range(1000,10000,1000))),result_sensitivity)\n",
    "sns.despine()\n",
    "plt.ylabel('Investment Return (%)',size = 14)\n",
    "plt.xlabel('Portfolio Size',size = 14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
